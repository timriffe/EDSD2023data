---
pagetitle: "Session 1"
title: | 
  | Data Wrangling for EDSDers
  | \vspace{1.5cm} \LARGE\emph{Session 1}
author: |
  | 13-16 Nov, 2023
  | Tim Riffe
  | Universidad del País Vasco \& Ikerbasque (Basque Foundation for Science)
date: "13 Nov, 2023"
output:
  html_document:
    number_sections: yes
    toc: yes
params:
  output_dir: "../EDSD2023data/docs"
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center}\includegraphics[trim=0 0 0 8cm, width=6cm, ]{assets/MPIDR_square_color.pdf}\\[\bigskipamount]}
- \posttitle{\end{center}}
bibliography: references.bib
---


<a href="https://github.com/timriffe/EDSD2022Data" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About me
				
Hi, I'm Tim Riffe, a US-American, [CED](https://ced.uab.cat/)-educated (Spain) demographer living in Spain and working at the [Universidad del País Vasco](https://www.ehu.eus/en/web/opik/tim-riffe) as an [Ikerbasque Research Fellow](https://www.ikerbasque.net/es/tim-riffe). I've been hooked on `R` since 2009 when I did [EDSD](https://www.eds-demography.org/), and have since authored several packages, mostly that do demographic things, but sometimes that do plotting things. My own research data preparation, analyses, analytic plotting, and presentation-quality plotting are all done in `R`. I've led dozens of introductory workshops like this one, including the previous four editions of the present EDSD module here: [2019](https://github.com/timriffe/EDSD2019data), [2020]([2019](https://github.com/timriffe/EDSD2020data), [2021](https://github.com/timriffe/EDSD2021data)), and [2022](https://github.com/timriffe/EDSD2022data)), However, **your module will be unique**. My own focuses are in programming tools for demographers, formal demography, data visualization, and health demography. Although this module is not about any of those things I hope we get a chance to talk about your work and my work if you're interested.
				
# About this module

The route to being an `R` data analyst could be via Base programming, as it was for me, but it is a longer road than simply jumping into the `tidyverse`. In this module we will use the [tidyverse](https://www.tidyverse.org/) framework to work with data. However, we'll also make use of function programming, and we'll blend the two things. Such code tends to be nicely legible, and in some ways it is easier to fix and change code. For a few different reasons, we'll be mostly working with R markdown and not in R scripts (although we could). The way to follow along in this course is to get set up correctly and to type along.

All I know is that you've had an intro course, and I assume no more than that. Where necessary I will review concepts. I expect you'll be able to keep up if you type along and take notes as we go, and if you ask questions and tell me to stop, slow down, or repeat when needed. Real learning will happen when you try to apply these concepts on your own, and for this reason I'll give exercises, which we'll then solve together. Often I will set myself up for apparent failure by not having worked through a solution in advance, and you'll see me induce errors and use help resources in a natural way. It's important to see this messy side of programming so that you also learn to make natural use of such resources and to react intelligently to error messages. 

This module consists in a bunch of worked examples, hopefully different enough from one another that by the end you will have mastered the basics, seen a good variety of functionality, and be able to extend your capabilities all by yourself. If you internalize this last bit then you can consider yourself empowered. The formal objective of this module is to get you able to construct _data processing pipelines_ to succinctly process haphazard messy datasets into tidy analyzable datasets via a series of sequential steps, including but limited to reading, parsing, recoding, _pivoting_, harmonizing, joining, grouping, aggregating, and so forth. By the end of the week, you'll be on your way to being able to fluently mix a large palette of these tools in a modular way. We do this course near the beginning of EDSD because these are skills you will use throughout the coming year(s). Practically, we'll mostly be using `dplyr` (and friends) to data wrangle, however any tool that can do the job is in-universe, so we'll see what happens. My _personal_ objective is to impart *attitude* and *confidence* when dealing with new and messy datasets. This class is a safe space, so please ask, complain, laugh, and join in my bewilderment at the contemporary datascape.

# R markdown basics

Class will happen in R markdown. Or maybe quarto? We should vote. Indeed, this material was produced in markdown. I'll say why in class and then fill this in.

# tidy data

*Tidy* data refers to tables of a particular layout, with one observation per row and one variable per column [@wickham2014tidy]. Tidy datasets processed using `tidyverse` tools allow for fast and understandable analyses that in many cases require no programming, whereas it often takes a certain amount of head-scratching (programming) to analyze not-tidy datasets. 

## Example of tidy data

For example, you're probably all familiar with HMD lifetables [@HMD]: This data is tidy. If you were to take all the HMD single age lifetables and stack them, adding new grouping columns for Sex and Country, then this would still be tidy. That is, a single HMD observation consists in a unique combination of Country, Sex, Year, and Age. Each lifetable column is a variable, so the tidy way to store them is the traditional way: in columns. 

## Examples of not tidy data.

1. The UN's DemoData database is not tidy: it's much longer than tidy data. All variables are stacked, and the variable type (qx, lx, counts, etc) is coded in an additional grouping column. This is efficient for storage, given that not every observation has every variable. It is not efficient for interactive programming because observations are potentially scattered over rows.

2. A matrix with ages in rows and years in columns can be very convenient for traditional programming, and it appeals to demographers' Lexis intuition, but it does not follow our definition of tidy. To make it tidy, we would need columns for age and year, and a new column containing the variable stored in the matrix.

3. Often panel data is delivered with each row representing an individual, and with many many variables. If there are 100 variables, and 10 waves, we end up with 1000 or so columns. For many analyses this is not considered tidy: an observation is a unique combination of individual and wave. To make it tidy, we'd need a column to store the wave variable, and each id would have a row for each wave where they were observed.

Different data structures exist for various reasons, but in the end tidy data facilitates the kind of thinking and operations needed for a particular system of flexible data analysis and visualization. The big question you need to answer is *what is an observation*, and this is determined by the target analysis.

## the gapminder `data.frame`

Let's work with a tidy `data.frame`. These are like a rectangular spreadsheet, or like a dataset in `Stata`: `data.frame`s have rows and columns, and there can be different kinds of data between columns, but only one kind of data within a column. Load it along with `tidyverse` in case you didn't before:

```{r}
library(gapminder)
library(tidyverse)
```

Or you could get metadata about the object from `str()` (structure), or `glimpse()` from the tidyverse. This tells us that we have 1704 observations (rows) of 6 variables, and then it tells us the variable (column) names, what kind of data it is (`Factor` = categorical, `int` = integer, `num` = numeric), and a sample of the first few observations in each.

```{r}
str(gapminder)
```

## Basic `ggplot2` with the gapminder data

This course isn't about `ggplot2`, but we'll use it from time to time, mostly so that there's a little prize at the end of each data processing pipeline. So we now have an explicit section on this will make this less mysterious.

### Overview

Tidy datasets such as this can also be visualized without further ado using a systematic grammar [@wilkinson2012grammar] implemented in the `ggplot2` package ( @wickham2016ggplot2, this loads automatically with `tidyverse`). The `gapminder` examples I'll give today and tomorrow are either gratuitously lifted or modified from @healy2018data, which you can either purchase as the book, or refer to the free version online: [www.socviz.co](www.socviz.co). It's a fabulous book. 

NOTE: when following along in these examples, you should **type along**, and not copy-paste the code.

Let's take a first look:

### *Map* to aesthetics

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() 
```

The `ggplot()` function *maps* variables in the `gapminder` dataset to either coordinates (x,y) or aesthetics (for example color). This sets up the plot metadata, but it does not plot the data because we didn't tell it how to do so: this final step is done by adding a point geometry `geom_point()`. 

Notes: The first argument specifies the dataset. The `mapping` argument is specified always as `mapping = aes()`, where *aes* stands for aesthetics. At a minimum you'll want to map to `x`, but in this case also `y` and `color`. The things that you can map are variables in the tidy dataset. And where necessary `ggplot2` guesses whether they are quantitative or categorical in nature. You need to give a mapping to `ggplot()`, but you can also give different mappings for different geoms specified directly in the `geom` functions. That only makes sense if you'll have more than one `geom` going into the plot (we'll see this). Further modifications to the coordinate base created by `ggplot()` are *added* in to the expression with `+`.

### Adjust axis scales

```{r}
library(scales)
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() +
	scale_x_log10(labels = dollar)
```

It turns out that by specifying a nice large set of possible mappings, coordinate systems, geoms, and discrete and continuous options for scaling mappings, that you can create just about any plot. For other tricky ones, there are usually packages available. Here's a nice overview of `ggplot2` functionality: [https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

### `geom`s and `facet`s

My objective now is to show you a few different geoms and ways of doing panels in `ggplot2`. 

For example, we can also add in a smoother to see the global trend:

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
	geom_point(mapping = aes(color = continent)) +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar)
```

See how geoms can also have their own special mapping? Had we left color in the first mapping, then everything that follows would have been split on continent. See:

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + 
	geom_point() +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar)
```

Wow, that's a noisy plot! What if instead of color we just split it into subplots by continent?

```{r}
ggplot(gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + 
	geom_point() +
	geom_smooth(method = "loess") +
	scale_x_log10(labels = dollar) + 
	facet_wrap(~continent)
```

We get panel plots like this by specifying a layout formula in `facet_wrap()`, where `~` separates left and right sides of the formula, and where left means rows in a panel layout and right means columns. Since there's nothing on the left it just orders the continents. Color is no longer needed since the groups are separated.

There is a time variable in this dataset that we've basically been ignoring. How about a `gdpPercap` time series?

Too busy?

```{r}
ggplot(gapminder, mapping = aes(x = year, y = gdpPercap, by = country)) +
	geom_line() 
```

Use faceting to separate the continents

```{r}
ggplot(gapminder, mapping = aes(x = year, y = gdpPercap, by = country)) +
	geom_line() +
	facet_wrap(~continent)
```

### Summary lessons 
More sophisticated usage of `ggplot2` builds on these basics in a rather consistent way. You can deepen your fluency by learning new `geoms`, learning what things can be *mapped* to aesthetics, and learning options to scale axes and mappings. To get an overview, refer to the `ggplot2` cheat sheet. For a very approachable introduction, try the Healy book. For the most thorough and advanced treatment, refer to the Wickham book. To learn usage, look at help examples, or R graph gallery examples (add links). In following lessons I will sometimes use `ggplot2`, and I will slip in further tips in an unstructured way.


# Basic `dplyr`
`ggplot2` is a wonderful tool if your data is already both *tidy* and *mappable*. But source data rarely is. To feed a dataset to `ggplot2` often we'll want to pre-process data, or even model it. The tools for data management, shaping, matching, sorting, joining, filtering, and so forth (stuff to prep for `ggplot2`) blend seamlessly into the tools of data analysis and modeling when one uses the `dplyr` approach. Some basic concepts will help us get started:

1. `dplyr` functions are *verbal* because they *do* things to data.
2. A bunch of `dplyr` functions executed in sequence on a dataset for a *pipeline*.
3. Data processing pipelines read as sentences, because verbs are strung together using pipes. This is the pipe symbol: `%>%`
4. Data processing pipelines are quick to build, easy to read, and help keep your `R` workspace less cluttered than most alternatives. Clean coding and clean workspaces means code is easier to maintain, even when not written by yourself!

This probably sounds jargony, so let's hop to it:

## Verbs {.tabset}
### Overview
Here are a selection of `dplyr` verbs. Some things to note: the first argument of each is a data object, usually something like a `data.frame`. These do things to the dataset, and they return the modified dataset back.

1. `group_by()` allows operations on subgroups
2. `pivot_longer()` make a wide range of columns to long format
3. `pivot_wider()` does the inverse
4. `mutate()` make new columns, potentially using other columns, no loss of rows
5. `summarize()` aggregates over rows. Usually reduces nr of rows
6. `select()` selects columns
7. `filter()` selects rows (subsets)


We'll see other `dplyr` verbs here and there during this module

Flip through the tabs in order.

#### `group_by()` 
When you want to operate on groups in a dataset, then it is necessary to declare the grouping.

```{r}
gap_grouped    <- group_by(gapminder, country, year, continent) 
head(gap_grouped)
```

Grouping does not do anything to the data directly. All values will be identical, and the dimensions are the same. It just adds a piece of metadata to the object that declares the grouping.

NB: the object created here is used in the following code chunk.

#### `pivot_longer()`

Here we use `pivot_longer()` to pull all the gapminder value columns into a single key-value pair, where the key is the variable column name, and value is the contents. The resulting `data.frame` is three times longer because there are three variable columns (and three key columns), and this mimics the setup of your own `DemoData` SQL database.

```{r, message = FALSE, warning = FALSE}
gapminder_long <- pivot_longer(gap_grouped, 
                         lifeExp:gdpPercap,
                         names_to = "varname",
                         values_to = "value") 
```

Note: `group_by()` is a necessary pre-step in order to maintain the unique key combinations. We could consider `continent` either a key or a variable. It's not strictly needed as a key because country-year uniquely identifies observations, but I left it there to not over-lengthen the dataset.

You can get back to the original format with the `pivot_wider()`, which is the opposite of `pivot_longer()`.

```{r, eval = FALSE}
pivot_wider(gapminder_long, names_from = "varname", values_from = "value")
```

#### `mutate()` 
`mutate()` adds columns to, or modifies columns in a `data.frame`/`tibble`. Think of the example of lifetable construction: You might start with deaths and exposures, and all other columns are created based on these. This is a case for `mutate()`. The main thing to remember is that `mutate()` does not change the number of rows. Keeping with our gapminder example, say we want to add a column for the mean life expectancy, but repeated for each row within each group. To get these means for each country, we'll want to group by country first (our previous grouping also grouped by year).

```{r}
gap_grouped <- group_by(gapminder, country)
gap_mutated <- mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp)) 
head(gap_mutated)
```

You can also just keep comma separating newly made columns, like so:

```{r, eval = FALSE}
mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp),
	   sd_lifeExp = sd(lifeExp),
	   cov_lifeExp = sd_lifeExp / mean_lifeExp,
	   cov_pop = sd(population) / mean(population)) 
```

You see how `cov_lifeExp` is created using columns being created within the same call? Pretty neat! Expressions can also be more complicated, see where `cov_pop` is created using two function calls.

NB: The object `gap_grouped` created here is also used in the next code chunk.

#### `summarize()`

If instead we wished to collapse the dataset to have one observation per country, then we use `summarize()` (this can also by lossy with respect to columns). Here is that first `mutate` expression when executed with `summarize()`:

```{r}
gap_summarize <- summarize(gap_grouped,
	   mean_lifeExp = mean(lifeExp))
head(gap_summarize)
dim(gap_summarize) # collapsed
```

If you want to summarize more than one column, then this works, also with sequential dependency over columns created on the fly:

```{r}
gap_summarize <- summarize(gap_grouped,
	   mean_lifeExp = mean(lifeExp),
	   mean_gdpPercap = mean(gdpPercap),
	   sd_gdpPercap = sd(gdpPercap),
	   cov_gdpPercap = sd_gdpPercap / mean_gdpPercap)
head(gap_summarize)
```

You can get a lot of mileage out of `group_by()`, `mutate()` and `summarize()`.

#### `select()` 
`select()` picks out columns, and potentially renames them:

```{r}
head(select(gapminder, country, GDP = gdpPercap))
```

If data are grouped, the key columns are retained (and it tells you so in the console):

```{r}
gap_grouped <- group_by(gapminder, country, year)
head(select(gap_grouped, GDP = gdpPercap))
```

#### `filter()` 

More often we want to subset rows to those that satisfy certain selection criteria. Use `filter()` for this:

```{r}
gap_filtered <- filter(gapminder,
					   year >= 2000,
					   year <= 2010,
					   continent == "Africa")
str(gap_filtered)
```

There are three logical expressions here, each producing a value of TRUE or FALSE for each row. Comma separation between these is interpreted as `&`, so the full logical expression evaluated is interpreted as: 
`year >= 2000 & year <= 2010 & continent == "Africa"`
Note that double equals `==` is for the logical evaluation of equality! The other logical operators are pretty straightforward. A vertical bar `|` is used for *or*.

* There are other packages in `R` that have functions called `filter()`, uh oh! If this ever comes up and causes problems for you, just write it as `dplyr::filter()` instead and it won't get mixed up.

#### `ungroup()` 

It is often a good idea to remove a grouping attribute from a data object when you're done using it. Otherwise you might operate within groups when you don't mean to. Then execute `ungroup()` on the object. This is also a clean way to remove a grouping before declaring a new and different one. For example, say we want to make a column for the mean `lifeExp` within the series for each country, and also the mean over countries within each year. Then we'll need to switch groupings:

```{r}
gap_gr1 <- group_by(gapminder, country)
gap_gr1 <- mutate(gap_gr1,
				  mean_lifeExp_country = mean(lifeExp))
# now undo to operate on a different group
gap_gr2 <- ungroup(gap_gr1)
gap_gr2 <- group_by(gap_gr2, year)
gap_gr2 <- mutate(gap_gr2, mean_lifeExp_year = mean(lifeExp))
head(gap_gr2)
```

### Pipes

I had to force myself not to use pipes in the previous section! Really `dplyr` functions can be executed in sequence without the need to create intermediate objects. If you executed the previous examples, look at how cluttered RStudio's environment tab has become. We can avoid all that by stringing things together using the pipe operator from the `magrittr` package that loads automatically with `tidyverse`. here are some of those example reworked. See how they read as sentences?

Before:

```{r, eval = FALSE}
gap_grouped <- group_by(gapminder, country)
gap_mutated <- mutate(gap_grouped,
	   mean_lifeExp = mean(lifeExp)) 
head(gap_mutated)
```

After:

```{r, eval = FALSE}
gapminder %>% 
	group_by(country) %>% 
	mutate(mean_lifeExp = mean(lifeExp)) %>% 
	head()
```

Note: the first argument (`data`) is automatically filled with the object running through the pipeline. You should imagine `gapminder` running through a staged pipeline, getting modified at each step. When it gets to the end we just show the first several rows in the console.

Before:

```{r, eval = FALSE}
gap_gr1 <- group_by(gapminder, country)
gap_gr1 <- mutate(gap_gr1,
				  mean_lifeExp_country = mean(lifeExp))
# now undo to operate on a different group
gap_gr2 <- ungroup(gap_gr1)
gap_gr2 <- group_by(gap_gr2, year)
gap_gr2 <- mutate(gap_gr2, mean_lifeExp_year = mean(lifeExp))
head(gap_gr2)
```

After:

```{r, eval = FALSE}
gapminder %>% 
	group_by(country) %>% 
    mutate(mean_lifeExp_country = mean(lifeExp)) %>% 
	ungroup() %>% 
    group_by(year) %>% 
    mutate(mean_lifeExp_year = mean(lifeExp)) %>% 
	head()
```

Instead of ending in `head()`, we could assign the result to a new object. This is usually done at the start, like so:

```{r, eval = FALSE}
gap_mutated <- 
	gapminder %>% 
	group_by(country) %>% 
    mutate(mean_lifeExp_country = mean(lifeExp)) %>% 
	ungroup() %>% 
    group_by(year) %>% 
    mutate(mean_lifeExp_year = mean(lifeExp)) 
```

The object `gap_mutated` consists in the fully-executed pipeline. 

Note: We usually construct pipelines like these incrementally, often checking as we to make sure things work as expected. Often that checking is done like we saw above, with ` %>% head()` or similar as the last step. As we determine next steps, this checker line gets pushed further down in the chain of execution. Just remember to remove it when you're done building the pipeline! Another great tool is the new `ViewPipeSteps` addin, which opens up a tab for each step in a pipeline! You need to install it from github like so:

```{r, eval = FALSE}
remotes::install_github("daranzolin/ViewPipeSteps")
```

Select this text, then from the addins menu in `RStudio`, select `View Pipe Chain Steps`
```{r}
library(ViewPipeSteps)
gapminder %>% 
	group_by(country) %>% 
  mutate(mean_lifeExp_country = mean(lifeExp)) %>% 
	ungroup() %>% 
  group_by(year) %>% 
  mutate(mean_lifeExp_year = mean(lifeExp)) 
```

Note also: We use indentation and alignment to make the code more legible.

As mentioned, `dplyr` pipelines also read as sentences. This last one can be verbalized as "first take the gapminder dataset, then group it by country, then make a new column for the average life expectancy over time within the country, then regroup by year and make a new column for the average life expectancy over countries within the year". Such sentences don't make for good lit but they do make your code accessible to readers, which is a major advantage. You can (should) also comment as you go.

Legibility of a script is achieved by reducing clutter and by using succinct pipelines. This is important because your processing/analysis code becomes all the more shareable, both with others and with future you. Who hasn't looked back at your own code written years ago and wonders what the heck it does? It feels awful to have to reverse engineer your own logic and coding. Yes, it can be overcome with good annotations, but let's obviate the need altogether if we can.

As mentioned, I'll slip in pipelines and `ggplot2` here and there in the coming days, so there will be several opportunities for practice and repetition.

# Exercises

# A `dplyr` worked example

Let's download this spreadsheet and figure out how to work with it! The exercise is to produce a [ridgeplot](https://www.google.com/search?q=ridgeplot&sxsrf=ACYBGNS1APa3kpQvaQUDckhfekTPMdzKjg:1573488533160&source=lnms&tbm=isch&sa=X&ved=0ahUKEwjj0c7IxeLlAhUKtRoKHauQDGcQ_AUIEigB&biw=1849&bih=932) where levels are TFR intervals, and two single age asfr distributions are shown, picked out as those that have the highest and lowest mean age at childbearing (MAB) respectively. Each code chunk is described right below it.

[http://data.un.org/DocumentData.aspx?id=319](http://data.un.org/DocumentData.aspx?id=319)

Download the spreadsheet and stick it in a new folder called `Data`. We're going to figure this thing out. For the intrepid, you can create the folder and automatically download the data like so:

```{r, eval = FALSE}
if (!dir.exists("Data")){
  dir.create("Data")
}
data_download_trigger_url <-
  "http://data.un.org/Handlers/DocumentDownloadHandler.ashx?id=319&t=bin"
# give R some extra time in case we have choppy connection
options(timeout = 200)
# download triggers properly with above url
download.file(url = data_download_trigger_url,
              destfile = "Data/un_fertility.xls")
```


```{r}
library(readxl)
ASFR <- read_excel(
  "Data/un_fertility.xls", 
    na = "..", 
    skip = 4) %>% # Ctrl + Shift + m
  select(1:13) %>% 
  dplyr::rename("Country" = "...1", "LocID" = "...2",
                     "PeriodVerbose" = "...3","TFR" = "...6",
                     "15" = "15-19","20" = "20-24",
                     "25" = "25-29","30" = "30-34",
                     "35" = "35-39","40" = "40-44",
                     "45" = "45-49")
```

Here we have a 3-step pipeline! Yay. The first step is syntax to read in from a spreadsheet, generated interactively using the import data feature of `RStudio.` Copy. Paste. Bam. Step 2 `select()`s just the columns we want for now. Step 3 `rename()`s the columns manually to something useful. 

Pipelines read like sentences. Because they are made of verbs and the pipe operator reads as *and then do this*.

```{r}
# here we're assigning at the top, but it's getting
# what comes out the bottom
ASFR <- ASFR %>% 
  select(-c(PeriodVerbose, TFR, Period)) %>% 
  pivot_longer(cols = `15`:`45`, 
               names_to = "Age",
               values_to = "asfr") %>% 
  mutate(Age = as.integer(Age),
         asfr = asfr / 1000) %>% 
  filter(!is.na(asfr)) %>% 
  group_by(Country, Year) %>% 
  mutate(TFR = sum(asfr, na.rm = TRUE) * 5,
         MAB = sum((5 * asfr) * (Age + 2.5), na.rm = TRUE) / TFR,
         # how many rows in this subset?
         n = n()) %>%
  ungroup() %>% 
  # just keep usable subsets
  filter(TFR > 0, 
         n > 4)  # ASFR up top gets what comes out here
```

What method do we use to split these data to single ages?

Suggestions from the field:

1. PCLM
2. Sprague (or Beers, or Grabill, or ...)
3. smooth spline through uniform repeated values in age groups (not constrained, but could work OK)
4. monotonic spline through cumulative distribution
5. use a fertility model, e.g. Coale-Trussel and pals
6. TOPALS [http://schmert.net/calibrated-spline/](http://schmert.net/calibrated-spline/) Super awesome love it

We will try the monotonic spline method because it seems easier to write fresh, even though you probably wouldn't use this as your first choice in real applications.

```{r}
mono_graduate <- function(Age5, asfr5){
  
  # assume NAs are 0s, sometimes these are found 
  # in the first or last age groups. 
  # Rather than setting to 0, we could also throw these out,
  # and adapt Age5 variable made below...
  asfr5[is.na(asfr5)] <- 0
  # anchor the endpoints (added double anchor)
  Age5                <- c(13,14, Age5+4, 50,51)
  asfr5               <- c(0,0, asfr5, 0,0)
  # scale up asfr
  asfr5               <- asfr5 * 5
  # accumualte it
  casfr5              <- cumsum(asfr5)
  # start at 14 because we need firs differences
  # to accumulate...
  predict_ages        <- 14:49
  # fit the spline
  xy <- splinefun(x = Age5,
            y = casfr5,
            method = "monoH.FC",
            ties = "min")(predict_ages)
  # return tibble
  tibble(Age = 15:49, 
         asfr = diff(xy))
}
chunk <- filter(ASFR, LocID == 4, Year == 1973)
plot(mono_graduate(Age5 = chunk$Age,
              asfr5 = chunk$asfr))
```

Great now we have a spline function, let's see if we can figure out how to use it in a pipeline.

```{r, message=FALSE}
ASFR1 <-
  ASFR %>% 
  group_by(Country, LocID, Year) %>% 
  group_modify(~mono_graduate(.x$Age, .x$asfr)) %>% 
  ungroup() 

nrow(ASFR)
nrow(ASFR1) # yup it's bigger now
```

Each subset now has single ages 15-49. We don't know yet whether there were any pathological cases. Let's make a scatterplot of MAB by TFR to find out.

```{r}
#install.packages("ggpointdensity")
library(ggpointdensity)
library(colorspace)
ASFR1 <-
  ASFR1 %>% 
  group_by(Country, LocID, Year) %>% 
  mutate(MAB = sum(Age * asfr) / sum(asfr),
            TFR = sum(asfr),
            .groups = "drop") 
ASFR1 %>% 
  filter(Age == 30) %>% 
  ggplot(mapping = aes(x = MAB, y = TFR)) +
  geom_pointdensity() + 
  scale_color_continuous_sequential(palette = "Reds")

```

Now we start with the exercise. We bin TFRs into some intervals, let's just say .25 or so. This can happen in a `mutate()` call, and we do it by subtracting the modulo using `%%` (super awesome operator). Next step is to group by `TFRint` and pick out the minumum and maximum MAB in each interval band.

```{r}

ASFRex <- ASFR1 %>% 
 # filter(MAB > 20) %>% 
  mutate(TFRint = TFR - TFR %% .25) %>% 
  group_by(TFRint) %>% 
  mutate(extremes = case_when(
                      MAB == max(MAB) ~ "max",
                      MAB == min(MAB) ~ "min",
                      TRUE ~ NA_character_
  )) %>% 
 filter(!is.na(extremes))

```

Now this is something we can plot! Let's use the `ggridges` package to bring in the ridgeplot geom.

```{r, fig.height=15}
#install.packages("ggridges")
library(ggridges)

ASFRex %>% 
  filter(TFRint >= 1,
         TFRint < 7.1) %>% 
  ggplot(mapping = aes(x = Age, 
                y = factor(TFRint, 
 						 		           levels = sort(unique(TFRint)), 
 						 		           ordered = TRUE))) + 
 	geom_ridgeline(mapping = aes(x = Age, 
 								               height = asfr, 
 								               fill = extremes),  
 	               scale = 4, 
 				         alpha = .6) +
  labs(y = "TFR")

```

We start by filtering out some of the extreme TFRs, which have few observations. Then we pass the main data object into `ggplot()`, mapping `Age` to `x` and `y` to `TFRint`. In this case, `y` means the ridge level, not the shifted height of each curve. Note since we want TFR to increase on the `y` axis, we need to set it as an ordered factor. This is a common ggplot2 trick, so not bad to see it. To draw the curves, we use the `geom_ridgeline()` geom, giving a new mapping there. Within the geom, `x` is `Age`, and `y` is `asfr`, and we finally map fill color to the extremes variable. 

Looking at the results, we note a few odd cases, which could be problematic spline fits, or poor data quality. If we wanted to pursue this any further then these would require closer examination. I'm afraid some of these series might have poor quality, or else data gaps that cause the splines to behave bad, not sure. We could also try the Schmertmann or some other method. Let's chalk this up as a win for now! 

# References

